{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Training CNNS.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm","authorship_tag":"ABX9TyNCaHKLKxtzlUh/zUHMEkRK"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"}},"cells":[{"cell_type":"code","metadata":{"colab_type":"code","executionInfo":{"status":"ok","timestamp":1587267823715,"user_tz":240,"elapsed":9038,"user":{"displayName":"Humza Hemani","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj8JpMBUqwhKFXiSZHjfH1UTcnRrDzWPKo5Lg41=s64","userId":"15690928755966606348"}},"id":"7VapKFPh-ehr","outputId":"743e3de8-63a1-4960-ec50-cbcba8adc046","colab":{"base_uri":"https://localhost:8080/","height":221}},"source":["!pip3 install torch\n","from sklearn import preprocessing\n","from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n","import time\n","import pathlib\n","import dill\n","import numpy as np\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (1.4.0)\n","Collecting skorch\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/42/21/4936b881b33de285faa0b36209afe4f9724a0875b2225abdc63b23d384a3/skorch-0.8.0-py3-none-any.whl (113kB)\n","\u001b[K     |████████████████████████████████| 122kB 2.9MB/s \n","\u001b[?25hRequirement already satisfied: tqdm>=4.14.0 in /usr/local/lib/python3.6/dist-packages (from skorch) (4.38.0)\n","Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from skorch) (1.4.1)\n","Requirement already satisfied: tabulate>=0.7.7 in /usr/local/lib/python3.6/dist-packages (from skorch) (0.8.7)\n","Requirement already satisfied: scikit-learn>=0.19.1 in /usr/local/lib/python3.6/dist-packages (from skorch) (0.22.2.post1)\n","Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from skorch) (1.18.2)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.19.1->skorch) (0.14.1)\n","Installing collected packages: skorch\n","Successfully installed skorch-0.8.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","executionInfo":{"status":"ok","timestamp":1587267845833,"user_tz":240,"elapsed":31139,"user":{"displayName":"Humza Hemani","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj8JpMBUqwhKFXiSZHjfH1UTcnRrDzWPKo5Lg41=s64","userId":"15690928755966606348"}},"id":"qRLBs-dkKD5l","outputId":"80bc1451-99c1-46d0-ca06-d165f3df747a","colab":{"base_uri":"https://localhost:8080/","height":122}},"source":["from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"OgPGU_aH-Q13","colab":{}},"source":["class downwardSlope(nn.Module):\n","    def __init__(self, maxSeqLength, outSize):\n","        super().__init__()\n","        convOutLength = maxSeqLength\n","        padding = 1\n","        dilation = 1\n","        kernel_size = 15\n","        stride = 1\n","\n","        self.conv1 = nn.Conv1d(13, 32, kernel_size=kernel_size, dilation =dilation ,stride=stride, padding=padding)\n","        #convOutLength calculated using equation in nn.conv1d documentation\n","        convOutLength = (convOutLength + 2*padding -dilation*(kernel_size-1) -1)//stride + 1\n","        self.conv2 = nn.Conv1d(32, 32, 15, padding=padding)\n","        convOutLength = (convOutLength + 2*padding -dilation*(kernel_size-1) -1)//stride + 1\n","        self.conv3 = nn.Conv1d(32, 32, 15, padding=padding)\n","        convOutLength = (convOutLength + 2*padding -dilation*(kernel_size-1) -1)//stride + 1\n","        self.maxPool = nn.MaxPool1d(convOutLength)\n","        self.fc1 = nn.Linear(32, outsize)\n","\n","    def forward(self,x):\n","        x= self.conv1(x)\n","        x= self.conv2(x)\n","        x= self.conv3(x)\n","        x = self.maxPool(x)\n","        x = x.reshape(x.shape[0],-1)\n","        x = self.fc1(x)\n","        x = torch.sigmoid(x)\n","        x = torch.squeeze(x)\n","        return x"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"Gmi7Bkvb-b68","colab":{}},"source":["def evaluateModel(model, lossCriterion, X, y, batchSize):\n","    with torch.no_grad():\n","        n=batchSize\n","        batched = list(zip([X[i:i + n] for i in range(0, len(X), n)],\n","                          [y[i:i + n] for i in range(0, len(y), n)]))\n","        predList = []\n","        yList = []\n","        lossMeans = []\n","        for i,(batchX, batchY) in enumerate(batched):\n","            #print('x',type(X), X.dtype)\n","            #print('batchx',type(batchX), batchX.dtype)\n","            predProb = model(batchX)\n","            if np.isnan(predProb.cpu().detach().numpy()).any():\n","                print(\"AAAAAA a NAN\")\n","                return\n","            loss = lossCriterion(predProb, batchY)\n","            batchPred = np.argmax(predProb, axis=1).tolist()\n","            predList = predList+batchPred\n","            batchYargmaxed = batchY.tolist()\n","            yList = yList+batchYargmaxed\n","            lossMeans.append(torch.mean(loss).item())\n","\n","        acc = accuracy_score(yList, predList)\n","        report = classification_report(yList, predList)\n","        confMat = confusion_matrix(yList, predList)\n","        lossMean = sum(lossMeans)/float(len(lossMeans))\n","\n","    return lossMean, confMat, acc, report"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"7e9Wtsca-dC0","colab":{}},"source":["def trainModel(model, optimizer, criterion, train_X, train_y, val_X, val_y, batchSize, startEpoch, endEpoch, modelName):\n","    notifyEvery = 5\n","\n","    checkmarkTime = time.time()\n","\n","    n=batchSize\n","    batched = list(zip([train_X[i:i + n] for i in range(0, len(train_X), n)],\n","                      [train_y[i:i + n] for i in range(0, len(train_y), n)]))\n","    numBatches = len(batched)\n","\n","    trainLosses= []\n","    valLosses =[]\n","    trainConfusionMatrices =[]\n","    valConfusionMatrices = []\n","    trainAccuracies = []\n","    valAccuracies = []\n","    trainReports = []\n","    valReports = []\n","\n","    out_folder = pathlib.Path(\"/content/drive/My Drive/validated_clips/model_data/\")\n","    out_folder = out_folder / modelName\n","    out_folder.mkdir(parents=True, exist_ok=True)\n","\n","    print(\"number of batches\", numBatches)\n","    for epoch in range(startEpoch, endEpoch):\n","        print(\"epoch:\", epoch)\n","        for i,(batchX,batchy) in enumerate(batched):\n","            optimizer.zero_grad()\n","            output =  model(batchX)\n","            loss = criterion(output, batchy)\n","            loss.backward()\n","            optimizer.step()\n","            if np.isnan(output.cpu().detach().numpy()).any():\n","                print(\"AAAAAA a NAN\")\n","                return\n","            if i%notifyEvery ==notifyEvery-1:\n","                print('[%d, %5d]' %\n","                  (epoch + 1, i + 1))\n","                timeTook = time.time() - checkmarkTime\n","                print(\"took\", timeTook, \"seconds for\", notifyEvery, \"batches\")\n","                if(torch.cuda.is_available()):\n","                    print(torch.cuda.max_memory_allocated()/1e9, \"GB of VRAM being used\")\n","                checkmarkTime = time.time()\n","        trainLoss, trainConfMat, trainAccuracy, trainReport = evaluateModel(model,criterion,train_X, train_y, batchSize)\n","        valLoss, valConfMat, valAccuracy, valReport = evaluateModel(model, criterion,val_X, val_y, batchSize)\n","        trainLosses.append(trainLoss)\n","        valLosses.append(valLoss)\n","        trainConfusionMatrices.append(trainConfMat)\n","        valConfusionMatrices.append(valConfMat)\n","        trainAccuracies.append(trainAccuracy)\n","        valAccuracies.append(valAccuracy)\n","        trainReports.append(trainReport)\n","        valReports.append(valReport)\n","        print('val loss', valLoss, \"train loss\", trainLoss)\n","        print(\"train conf mat\", trainConfMat)\n","        print(\"val conf mat\", valConfMat)\n","\n","        epoch_filename = \"epoch\"+str(epoch)+\".pkl\"\n","        modelEpochPath = out_folder / epoch_filename\n","        with modelEpochPath.open(\"wb\") as f:\n","          torch.save(model, f, pickle_module=dill)\n","    print(\"finished training\")\n","    \n","    trainLosses_path = out_folder / \"trainLosses.pkl\"\n","    with trainLosses_path.open(\"wb\") as f:\n","      dill.dump(trainLosses, f)\n","    valLosses_path = out_folder/\"valLosses.pkl\"\n","    with valLosses_path.open(\"wb\") as f:\n","      dill.dump(valLosses, f)\n","    trainConfusionMatrices_path = out_folder/\"trainConfustionMatrices.pkl\"\n","    with trainConfusionMatrices_path.open(\"wb\") as f:\n","      dill.dump(trainConfusionMatrices, f)\n","    valConfusionMatrices_path = out_folder/\"valConfustionMatrices.pkl\"\n","    with valConfusionMatrices_path.open(\"wb\") as f:\n","      dill.dump(valConfusionMatrices, f)\n","    \n","    trainAccuracies_path = out_folder / \"trainAccuracies.pkl\"\n","    with trainAccuracies_path.open(\"wb\") as f:\n","      dill.dump(trainAccuracies, f)\n","    valAccuracies_path = out_folder/\"valAccuracies.pkl\"\n","    with valAccuracies_path.open(\"wb\") as f:\n","      dill.dump(valAccuracies, f)\n","    trainReports_path = out_folder/\"trainReports.pkl\"\n","    with trainReports_path.open(\"wb\") as f:\n","      dill.dump(trainReports, f)\n","    valReports_path = out_folder/\"valReports.pkl\"\n","    with valReports_path.open(\"wb\") as f:\n","      dill.dump(valReports, f)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","executionInfo":{"status":"ok","timestamp":1587270518407,"user_tz":240,"elapsed":28436,"user":{"displayName":"Humza Hemani","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj8JpMBUqwhKFXiSZHjfH1UTcnRrDzWPKo5Lg41=s64","userId":"15690928755966606348"}},"id":"mdyt9GHRKVDV","outputId":"f9bab841-8df3-41d7-f88e-4ae127d8a462","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["balanced_classes_folder = pathlib.Path(\"/content/drive/My Drive/validated_clips/balanced_classes/\")\n","\n","test_path = balanced_classes_folder / \"test_data.pkl\"\n","val_path = balanced_classes_folder / \"val_data.pkl\"\n","train_path = balanced_classes_folder / \"train_data.pkl\"\n","\n","with open(test_path , \"rb\") as f:\n","  test_data = dill.load(f)\n","with open(val_path , \"rb\") as f:\n","  val_data = dill.load(f)\n","with open(train_path , \"rb\") as f:\n","  train_data = dill.load(f)\n","\n","train_X = torch.as_tensor(train_data['mfccs'], dtype=torch.float)\n","val_X = torch.as_tensor(val_data['mfccs'], dtype=torch.float)\n","print(train_X.shape)\n","train_X = train_X.permute(0,2,1)\n","val_X = val_X.permute(0,2,1)\n","\n","train_labels = train_data['labels']\n","val_labels = val_data['labels']\n","\n","le = preprocessing.LabelEncoder()\n","enc =  preprocessing.OneHotEncoder(handle_unknown='ignore')\n","le.fit(train_labels)\n","\n","val_y = le.transform(val_labels)\n","#val_y =enc.fit_transform(val_y.reshape(-1,1)).toarray() \n","val_y = torch.as_tensor(val_y, dtype=torch.long)\n","\n","train_y = le.transform(train_labels)\n","#train_y =enc.fit_transform(train_y.reshape(-1,1)).toarray()\n","train_y = torch.as_tensor(train_y, dtype=torch.long)"],"execution_count":83,"outputs":[{"output_type":"stream","text":["torch.Size([1040, 6234, 13])\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"ehKmTbpq--TL","colab":{"base_uri":"https://localhost:8080/","height":85},"outputId":"12758867-bc17-4a1e-a712-8d9b1e651c07","executionInfo":{"status":"ok","timestamp":1587270608599,"user_tz":240,"elapsed":462,"user":{"displayName":"Humza Hemani","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj8JpMBUqwhKFXiSZHjfH1UTcnRrDzWPKo5Lg41=s64","userId":"15690928755966606348"}}},"source":["seqLength = train_X.shape[2]\n","outsize = len(le.classes_)\n","print( seqLength, outsize)\n","mod = downwardSlope(seqLength, outsize)\n","\n","if (torch.cuda.is_available()):\n","  device = \"cuda:0\"\n","else:\n","  device = \"cpu\"\n","  print(\"device is\",device)\n","\n","mod = mod.to(device)\n","\n","criterion = nn.CrossEntropyLoss()\n","endEpoch = 3\n","learningRate = 0.001\n","optimizer = torch.optim.Adam(mod.parameters(), lr = learningRate)\n","\n","startEpoch = 0\n","endEpoch = 3\n","batchSize = 100\n","modelName = \"initial\"\n","\n","print('train_x',type(train_X), train_X.dtype)\n","print('train_y',type(train_y), train_y.dtype, train_y.shape)"],"execution_count":88,"outputs":[{"output_type":"stream","text":["6234 4\n","device is cpu\n","train_x <class 'torch.Tensor'> torch.float32\n","train_y <class 'torch.Tensor'> torch.int64 torch.Size([1040])\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"HryMmt2S3r0X","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"870d63c2-d198-4b9c-a04c-d3bbcd3ee88d","executionInfo":{"status":"ok","timestamp":1587271165575,"user_tz":240,"elapsed":116708,"user":{"displayName":"Humza Hemani","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj8JpMBUqwhKFXiSZHjfH1UTcnRrDzWPKo5Lg41=s64","userId":"15690928755966606348"}}},"source":["trainModel(mod, optimizer,criterion, train_X, train_y, val_X, val_y, batchSize, startEpoch, endEpoch, modelName)"],"execution_count":108,"outputs":[{"output_type":"stream","text":["number of batches 11\n","epoch: 0\n","<class 'torch.Tensor'>\n","<class 'torch.Tensor'>\n","<class 'torch.Tensor'>\n","<class 'torch.Tensor'>\n","<class 'torch.Tensor'>\n","[1,     5]\n","took 17.15275287628174 seconds for 5 batches\n","<class 'torch.Tensor'>\n","<class 'torch.Tensor'>\n","<class 'torch.Tensor'>\n","<class 'torch.Tensor'>\n","<class 'torch.Tensor'>\n","[1,    10]\n","took 14.264772415161133 seconds for 5 batches\n","<class 'torch.Tensor'>\n","epoch done\n","trainx shape torch.Size([1040, 13, 6234]) train_y shape torch.Size([1040])\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"],"name":"stderr"},{"output_type":"stream","text":["val loss 1.6477762460708618 train loss 1.5308151028373025\n","train conf mat [[  0   0 241  24]\n"," [  0   0 215  42]\n"," [  0   0 197  63]\n"," [  0   0 196  62]]\n","val conf mat [[ 0  0 28  6]\n"," [ 0  0 31  3]\n"," [ 0  0 21 11]\n"," [ 0  0 31  5]]\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/serialization.py:360: UserWarning: Couldn't retrieve source code for container of type downwardSlope. It won't be checked for correctness upon loading.\n","  \"type \" + obj.__name__ + \". It won't be checked \"\n"],"name":"stderr"},{"output_type":"stream","text":["epoch: 1\n","<class 'torch.Tensor'>\n","<class 'torch.Tensor'>\n","<class 'torch.Tensor'>\n","<class 'torch.Tensor'>\n","<class 'torch.Tensor'>\n","[2,     5]\n","took 23.228028059005737 seconds for 5 batches\n","<class 'torch.Tensor'>\n","<class 'torch.Tensor'>\n","<class 'torch.Tensor'>\n","<class 'torch.Tensor'>\n","<class 'torch.Tensor'>\n","[2,    10]\n","took 18.467979669570923 seconds for 5 batches\n","<class 'torch.Tensor'>\n","epoch done\n","trainx shape torch.Size([1040, 13, 6234]) train_y shape torch.Size([1040])\n","val loss 1.5461490750312805 train loss 1.5173856236717918\n","train conf mat [[  0   0 259   6]\n"," [  0   5 234  18]\n"," [  0   0 230  30]\n"," [  0   0 228  30]]\n","val conf mat [[ 0  0 32  2]\n"," [ 0  0 31  3]\n"," [ 0  0 26  6]\n"," [ 0  0 34  2]]\n","epoch: 2\n","<class 'torch.Tensor'>\n","<class 'torch.Tensor'>\n","<class 'torch.Tensor'>\n","<class 'torch.Tensor'>\n","<class 'torch.Tensor'>\n","[3,     5]\n","took 19.846147298812866 seconds for 5 batches\n","<class 'torch.Tensor'>\n","<class 'torch.Tensor'>\n","<class 'torch.Tensor'>\n","<class 'torch.Tensor'>\n","<class 'torch.Tensor'>\n","[3,    10]\n","took 17.34526300430298 seconds for 5 batches\n","<class 'torch.Tensor'>\n","epoch done\n","trainx shape torch.Size([1040, 13, 6234]) train_y shape torch.Size([1040])\n","val loss 1.5561805963516235 train loss 1.513116110454906\n","train conf mat [[  0   0 260   5]\n"," [  0   7 236  14]\n"," [  0   0 241  19]\n"," [  0   1 239  18]]\n","val conf mat [[ 0  0 32  2]\n"," [ 0  0 33  1]\n"," [ 0  0 30  2]\n"," [ 0  0 35  1]]\n","finished training\n"],"name":"stdout"}]}]}